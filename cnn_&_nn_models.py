# -*- coding: utf-8 -*-
"""CNN_&_NN_models

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18SpcOo8OArNDsPdJBLLv77QnwAUb6hZb
"""

!pip install pyshark
!pip install tensorflow

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv1D, Conv2D, MaxPooling1D
from keras.optimizers import Adam
from keras.utils import to_categorical
from keras import regularizers
from sklearn.preprocessing import MinMaxScaler

#                                                                  ~~~~~~  PART 1  ~~~~~~~
# This is part one - analysis of normal transmission to check a possibility of DDos Attack. This uses the number of packets/time frame to get the data.
# However this could be very top level analysis and doesnt go deep.

# Reading the CSV File
df = pd.read_csv('ddos_packet_for_firewall.csv' , encoding = 'latin-1')

# Filtering all the packets.
tcp_df = df[df['Protocol'].isin(['TCP', 'TLSv1.2'])]
udp_df = df[df['Protocol'].isin(['UDP', 'DNS', 'MDNS', 'QUIC', 'SSDP'])]
arp_df = df[df['Protocol'] == 'ARP']
other_df = df[~df['Protocol'].isin(['TCP', 'TLSv1.2', 'UDP', 'DNS', 'MDNS', 'QUIC', 'SSDP', 'ARP'])]

# Printing total number of packets
print(f'Total number of packets: {len(df)}')
print()

# Printing number of TCP, UDP, ARP and other packets
print(f'Number of TCP packets: {len(tcp_df)}')
print(f'Number of UDP packets: {len(udp_df)}')
print(f'Number of ARP packets: {len(arp_df)}')
print(f'Number of other packets: {len(other_df)}')
print()

# Printing other important data
print(f'Top 5 source addresses for TCP packets:\n{tcp_df["Source"].value_counts().head()}')
print()
print(f'Top 5 destination addresses for TCP packets:\n{tcp_df["Destination"].value_counts().head()}')
print()
print(f'Maximum packet length for TCP packets: {tcp_df["Length"].max()} bytes')
print()

# Checking for all columns present and types of value.
print(df.head())

print(df['Time']) # To check the time frame.

#Convert to DateTime Format to compare the values.
df['Time'] = pd.to_datetime(df['Time'])
print(df['Time'])

time_frame_size = 0.001 # This is going to be very small value. In milliseconds
flag = 0
# Starting and Ending time for each time frame is calculated here.
start_times = pd.date_range(start=df['Time'].iloc[0], end=df['Time'].iloc[-1], freq=f"{time_frame_size}us")
end_times = start_times + pd.Timedelta(milliseconds=time_frame_size)

# Store the packet count for each time frame
packet_counts = {}

# Iterate through it
for i in range(len(start_times)):
    mask = (df['Time'] >= start_times[i]) & (df['Time'] < end_times[i])
    count = df.loc[mask].shape[0]
    packet_counts[f"{start_times[i]} to {end_times[i]} seconds"] = count

# Printing the results
for time_frame, packet_count in packet_counts.items():
    print(f"{time_frame} -> {packet_count} packets")
    if (packet_count >= 50000):
      flag = 1

if (flag == 1):
  print("DDOS Detected")
else:
  print("DDOS Not Detected")

# Conclusion : A lot of packets were transferred between 0.000000600 to 0.000000700 which indicate a lot of packets were tried to be send to the dest ip.

#                                                                  ~~~~~~  PART 2  ~~~~~~~
# This is part two - analysis of normal transmission to check details like intervals, TX mode, Size and Sequence Data , CPU , Memory.
# These are important for analysis as well.

# Generated the part2.csv with this linux.
# tshark -i wlan0 -T fields -E separator=, -E header=y -e frame.time_relative -e eth.dst -e eth.src -e ip.src -e ip.dst -e ip.proto -e tcp.srcport -e tcp.dstport -e udp.srcport -e udp.dstport -e frame.len -e tcp.seq -e tcp.flags -e tcp.analysis.retransmission -e tcp.analysis.out_of_order -e tcp.analysis.fast_retransmission > part2.csv

df = pd.read_csv('/content/part2_Mem_Cpu.csv')

# Time interval between two consecutive packets are calculated here
df['Interval'] = df['frame.time_relative'].diff()

# Transmission mode based on the destination MAC address (However not very accurate)
df['TX mode'] = 'unicast'
df.loc[df['eth.dst'].str.contains('ff:ff:ff:ff:ff:ff|01:00:5e|33:33'), 'TX mode'] = 'multicast/broadcast'

# Size of the packet in bytes
df['Size'] = df['frame.len']

# TCP sequence number
df.loc[df['ip.proto'] == 6, 'Sequence data'] = df['tcp.seq']

print(df[['Interval', 'TX mode', 'Size', 'Sequence data']])

df = pd.read_csv('/content/part2_Mem_Cpu.csv')

# Convert the hexadecimal tcp flags to integer (TO avoid type errors)
df['tcp.flags'] = df['tcp.flags'].apply(lambda x: int(str(x), 16) if isinstance(x, str) else x)
df['tcp.flags'] = df['tcp.flags'].fillna(0).astype(int)

# Packet time interval
df['time_interval'] = df['frame.time_relative'].diff()

# TCP retransmissions
df['tcp_retransmissions'] = df['tcp.flags'] & 0x010

# TCP segments with the urgent pointer set
df['tcp_urgent_pointer'] = df['tcp.flags'] & 0x020

# Estimation CPU usage based on the number of TCP retransmissions
cpu_usage = df['tcp_retransmissions'].sum() / df['time_interval'].sum()

# Estimate memory usage based on the number of TCP segments with the urgent pointer set
memory_usage = df['tcp_urgent_pointer'].sum() / df['time_interval'].sum()

# Print the estimated CPU and memory usage
print('CPU usage:', cpu_usage)
print('Memory usage:', memory_usage)

# Conclusion : There was a heavy CPU Usage detected in the packet flow. Thus there might be a chance of flooding attack.

#                                                                  ~~~~~~  PART 3  ~~~~~~~
# Final part includes the Best way to find a DDos Attack using CNN.

df = pd.read_csv('/content/part3_ddos_data.csv')

# Drop any rows with null values
df.dropna(inplace=True)

# To avoid any unwanted spaces in header
df.columns = [col.strip() for col in df.columns]

# Important Columns
df = df[['Destination Port', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets', 'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Packets/s', 'Bwd Packets/s', 'Min Packet Length', 'Max Packet Length', 'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Average Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size', 'Fwd Header Length', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward', 'act_data_pkt_fwd', 'min_seg_size_forward', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label']]

# Label Encoder
df['Label'] = df['Label'].map({'BENIGN': 0, 'DDoS': 1})

# Forming new preprocessed Dataset
df.to_csv('preprocessed_ddos.csv', index=False)

df.head()

df = pd.read_csv("preprocessed_ddos.csv")

# Check for NaN and infinity values
print(np.isnan(df).sum()) # 0
print(np.isinf(df).sum()) # 60

# Replace infinity values with a large number
df.replace([np.inf, -np.inf], np.nan, inplace=True)
df.fillna(1e9, inplace=True)

# Normalize the input data (For better accuracy, shoudnt have any other datatypes. Thus did the above steps)
scaler = MinMaxScaler() # give values in range of 0 to 1
X = scaler.fit_transform(df.iloc[:, :-1].values)

"""

```
# This is formatted as code
```

**1D CNN ARCHITECTURE**"""

# Split the data into training and testing sets
y = to_categorical(df.iloc[:, -1].values)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# CNN model # sigmoid # tanh # relu
model = Sequential()
model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))
model.add(MaxPooling1D(pool_size=2))
model.add(Dropout(0.25))
model.add(Conv1D(64, kernel_size=3, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dense(2, activation='softmax', kernel_regularizer=regularizers.l2(0.01)))

# Model Compilation
model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])

# Fit the model to the training data
model.fit(X_train.reshape(X_train.shape[0], X_train.shape[1], 1), y_train, batch_size=128, epochs=4)

# Testing data
loss, accuracy = model.evaluate(X_test.reshape(X_test.shape[0], X_test.shape[1], 1), y_test, verbose=1)
print("Test accuracy:", accuracy)

accuracy * 100 # percentage

# Machine Learning models can predict DDos Attacks with higher Accuracy

"""**NN** **ARCHITECTURE**"""

#NN
from keras.utils import to_categorical

# Split the data into training and testing sets
y = to_categorical(df.iloc[:, -1].values)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# NN model with 2 hidden layers
model = Sequential()
model.add(Dense(32, activation='relu', input_shape=(X_train.shape[1],)))
model.add(Dropout(0.25))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dense(2, activation='softmax', kernel_regularizer=regularizers.l2(0.01)))

# Model Compilation
model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])

# Fit the model to the training data
model.fit(X_train, y_train, batch_size=128, epochs=2)

# Testing data
loss, accuracy = model.evaluate(X_test, y_test, verbose=1)
print("Test accuracy:", accuracy)

accuracy * 100 # percentage

"""**TRAINED 2D CNN MODEL**"""

from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
import numpy as np
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalMaxPooling2D
from keras.optimizers import Adam

df = pd.read_csv("preprocessed_ddos.csv")

# Check for NaN and infinity values
print(np.isnan(df).sum()) # 0
print(np.isinf(df).sum()) # 60

# Replace infinity values with a large number
df.replace([np.inf, -np.inf], np.nan, inplace=True)
df.fillna(1e9, inplace=True)

# Normalize the input data (For better accuracy, shoudnt have any other datatypes. Thus did the above steps)
scaler = MinMaxScaler() # give values in range of 0 to 1
X = scaler.fit_transform(df.iloc[:, :-1].values)

# Split the data into training and testing sets
y = to_categorical(df.iloc[:, -1].values)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1, 1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1, 1)

# CNN model
model = Sequential()
model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(X_train.shape[1], 1, X_train.shape[2]), strides=(1,1), padding='same'))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))
model.add(Dropout(0.25))
model.add(Conv2D(64, kernel_size=(3,3), activation='relu', strides=(1,1), padding='same'))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))
model.add(Dropout(0.25))
model.add(GlobalMaxPooling2D())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dense(2, activation='softmax', kernel_regularizer=regularizers.l2(0.01)))

# Model Compilation
model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])

# Fit the model to the training data
model.fit(X_train, y_train, batch_size=128, epochs=9)

# Testing data
loss, accuracy = model.evaluate(X_test, y_test, verbose=1)
print("Test accuracy:", accuracy)

accuracy * 100 # percentage